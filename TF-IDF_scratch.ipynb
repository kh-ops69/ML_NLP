{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOwMRFeFL23sl88yP0VPL5f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yVyaBLLSOpPm","executionInfo":{"status":"ok","timestamp":1681971739128,"user_tz":-330,"elapsed":2139,"user":{"displayName":"Krish","userId":"13929302201026852395"}},"outputId":"3126d6ac-ae14-47c9-b3c7-f1c374cd2fd5"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')      # tokenizer package that helps divide a large piece of text into sentences or words or abbreviations\n","#punkt is also nltk's default word tokenizer package"]},{"cell_type":"code","source":["!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBqeVgloPd17","executionInfo":{"status":"ok","timestamp":1681971745221,"user_tz":-330,"elapsed":899,"user":{"displayName":"Krish","userId":"13929302201026852395"}},"outputId":"866c4eea-5b70-4104-e7b9-c051d0a4adf8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-04-20 06:22:24--  https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv\n","Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.21.23.210, 172.67.213.166, 2606:4700:3030::ac43:d5a6, ...\n","Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.21.23.210|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5085081 (4.8M) [text/csv]\n","Saving to: ‘bbc_text_cls.csv’\n","\n","bbc_text_cls.csv    100%[===================>]   4.85M  --.-KB/s    in 0.08s   \n","\n","2023-04-20 06:22:24 (58.1 MB/s) - ‘bbc_text_cls.csv’ saved [5085081/5085081]\n","\n"]}]},{"cell_type":"code","source":["text_df = pd.read_csv('bbc_text_cls.csv')\n","text_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"NMA0kyFNPibc","executionInfo":{"status":"ok","timestamp":1681971750523,"user_tz":-330,"elapsed":1041,"user":{"displayName":"Krish","userId":"13929302201026852395"}},"outputId":"0631e673-9aea-4823-f763-6f1e732d4c8c"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text    labels\n","0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n","1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n","2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n","3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n","4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"],"text/html":["\n","  <div id=\"df-27e6dc8c-0aa6-4941-861f-fc5abd827b35\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n","      <td>business</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n","      <td>business</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n","      <td>business</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n","      <td>business</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n","      <td>business</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27e6dc8c-0aa6-4941-861f-fc5abd827b35')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-27e6dc8c-0aa6-4941-861f-fc5abd827b35 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-27e6dc8c-0aa6-4941-861f-fc5abd827b35');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["text_df['labels'].unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXVt9kCNhhMF","executionInfo":{"status":"ok","timestamp":1681975040758,"user_tz":-330,"elapsed":755,"user":{"displayName":"Krish","userId":"13929302201026852395"}},"outputId":"c849cd66-a77b-4bbe-c85a-2dab5ffb484c"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['business', 'entertainment', 'politics', 'sport', 'tech'],\n","      dtype=object)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["text_df['text'].sample(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Sed8BzeWuZ4","executionInfo":{"status":"ok","timestamp":1681972245410,"user_tz":-330,"elapsed":1202,"user":{"displayName":"Krish","userId":"13929302201026852395"}},"outputId":"4145991d-baf0-4c7d-f03f-9cb4f2cfbb6e"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1063    MPs' murder sentence concern\\n\\nMurder sentenc...\n","398     Indian oil firm eyes Yukos assets\\n\\nIndia's b...\n","389     Wal-Mart to pay $14m in gun suit\\n\\nThe world'...\n","363     Durex maker SSL awaits firm bid\\n\\nUK condom m...\n","1360    Greek pair set for hearing\\n\\nKostas Kenteris ...\n","542     DVD review: Harry Potter and the Prisoner of A...\n","209     Gold falls on IMF sale concerns\\n\\nThe price o...\n","1889    Kenyan school turns to handhelds\\n\\nAt the Mbi...\n","1861    Movie body hits peer-to-peer nets\\n\\nThe movie...\n","1497    Candela completes Bolton switch\\n\\nBolton boss...\n","Name: text, dtype: object"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# word-to-index mapping, necessary while making count vectorizer function later, because we cannot make a tf idf thing without knowing the counts of each token in the documents, which is what count vectorizer does\n","word_to_id = {}\n","id = 0\n","tokenized_text = []         # to store the list of tokenized documents i.e. storing those document names which have already undergone the tokenization process\n","for tokens in text_df['text']:\n","  token = word_tokenize(tokens.lower())\n","  document_to_id = []\n","  for t in token:\n","    if t not in word_to_id:\n","      word_to_id[t] = id                  # t is the word, id is the number which we will use for our reference further\n","      id += 1\n","    \n","    document_to_id.append(word_to_id[t])              # saving each word as index in a separate list for later usage\n","  tokenized_text.append(document_to_id)              # all the tokenized text now stored in separate lists\n","    "],"metadata":{"id":"jPifZY2wQcKN","executionInfo":{"status":"ok","timestamp":1681976496025,"user_tz":-330,"elapsed":8657,"user":{"displayName":"Krish","userId":"13929302201026852395"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# reverse index-to-word mapping\n","rev_map = {value:key for key, value in word_to_id.items()}               # therefore here, the key is the word/token itself and the value is the number we allocated to it during making the word-to-index mapping\n","# rev_map\n","# we can also store all the key value pairs using lists, store the words using the value from word_to_id dict in the form of the array's indexes"],"metadata":{"id":"4Xl6Z6nHT0B8","executionInfo":{"status":"ok","timestamp":1681976540140,"user_tz":-330,"elapsed":427,"user":{"displayName":"Krish","userId":"13929302201026852395"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# each text is considered as a separate document \n","num_of_docs = len(text_df['text'])\n","num_of_docs"],"metadata":{"id":"GS3p3LtaT4ph","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681973427536,"user_tz":-330,"elapsed":378,"user":{"displayName":"Krish","userId":"13929302201026852395"}},"outputId":"ae26df00-6e14-47d9-dc1e-5dab77f9d4e6"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2225"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# number of words to be considered\n","num_of_words = len(word_to_id)\n","num_of_words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VY2f4QpnbcMq","executionInfo":{"status":"ok","timestamp":1681973506005,"user_tz":-330,"elapsed":1013,"user":{"displayName":"Krish","userId":"13929302201026852395"}},"outputId":"bd459a3e-19b5-4218-ea29-228cb82061de"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["38787"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# make a dense matrix(sparse matrix would be better), taking the number of docs and number of words as the dimensions for the matrix\n","array = np.zeros((num_of_docs, num_of_words))         "],"metadata":{"id":"CawBXEV7bLvj","executionInfo":{"status":"ok","timestamp":1681973586205,"user_tz":-330,"elapsed":762,"user":{"displayName":"Krish","userId":"13929302201026852395"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# now we populate our newly created array using enumerate\n","for i, document_to_id in enumerate(tokenized_text):               #tokenized text is the whole thing, first take each document separately using document_to_id\n","  for j in document_to_id:\n","    array[i,j] += 1 "],"metadata":{"id":"4J0xp8SycH0W","executionInfo":{"status":"ok","timestamp":1681973881822,"user_tz":-330,"elapsed":1688,"user":{"displayName":"Krish","userId":"13929302201026852395"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["document_freq = np.sum(array>0, axis = 0)            # need to find out how much frequency for each word in each doc, using axis = 0 it will give us the sum of word occurrences (if value exists, it is true, which is basically evaulated using array>0)\n","idf = np.log(num_of_docs/document_freq)                 # this is inverse document frequency formula for a term"],"metadata":{"id":"n-bqUHfedNAx","executionInfo":{"status":"ok","timestamp":1681976343040,"user_tz":-330,"elapsed":358,"user":{"displayName":"Krish","userId":"13929302201026852395"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["tf_idf = array*idf"],"metadata":{"id":"SXSWvMUbjlB2","executionInfo":{"status":"ok","timestamp":1681976914612,"user_tz":-330,"elapsed":994,"user":{"displayName":"Krish","userId":"13929302201026852395"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["np.random.seed(123)               # its a pseudo-random generator generating random numbers that arent truly random because they are determined by algorithm"],"metadata":{"id":"MVDW7AurgpsA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_document = np.random.choice(num_of_docs)\n","row = text_df.iloc[test_document]                # if we take round brackets instead of square here, it assumes the given parameter to be axis, not index\n","print(\"label= \", row['labels'])\n","print(\"text= \", row['text'].split('\\n', 1)[0])          # the text in df contains some \\n characters, so we split the sentences in that and retrieve the first sentence(title) from that location\n","print('top 5 terms:')\n","\n","scores = tf_idf[test_document]          # we need tf_idf score of test_document\n","indices = (-scores).argsort()\n","\n","for j in indices[:10]:\n","  if rev_map[j].isnumeric():\n","    rev_map.pop[j]\n","  # for i in rev_map[j]:\n","  #   if i.isdigit():\n","  #     rev_map[j].replace(i, ' ')\n","  print(rev_map[j])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"euVqEDRwg0t8","executionInfo":{"status":"ok","timestamp":1681977692927,"user_tz":-330,"elapsed":465,"user":{"displayName":"Krish","userId":"13929302201026852395"}},"outputId":"a100d74c-4725-400f-c76b-c1acda490b19"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["label=  business\n","text=  Cannabis hopes for drug firm\n","top 5 terms:\n","riyadh\n","essentially\n","follow-up\n","malta\n","year-ago\n","£5.38bn\n","addressed\n","looked\n","fifth-largest\n","panicked\n"]}]},{"cell_type":"markdown","source":["using scipy's CSR matrix, and another part to initialize counts using count vectorizer "],"metadata":{"id":"NKK30YlNufcG"}}]}